{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection of Natural Disasters in Brazil Across 10 years (if possible with Reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from secretcodes import reddit_secret, reddit_client, open_api_key\n",
    "\n",
    "# configure your API credentials\n",
    "reddit = praw.Reddit(\n",
    "    client_id=reddit_client,\n",
    "    client_secret=reddit_secret,\n",
    "    user_agent=\"brazil_disasters_scraper by u/haleyhernan\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_terms = [\n",
    "    \"natural disaster\", \"flood\", \"landslide\", \"brazil\", \"mudslide\", \"rockslide\",\n",
    "    \"tragedy\", \"emergency\", \"heavy rain\", \"storm\", \"road blocked\", \"evacuate\",\n",
    "    \"catastrophe\", \"hazard\", \"crisis\", \"search and rescue\", \"disaster zone\",\n",
    "    \"road closure\", \"highway closed\", \"emergency response\", \"disaster relief\",\n",
    "    \"intense\", \"torrential\", \"severe\", \"extreme\", \"floodwaters\", \"flash flood\",\n",
    "    \"collapse\", \"destruction\", \"damage\", \"injured\", \"dead\", \"casualties\",\n",
    "    \"missing\", \"fatalities\", \"mountain\", \"hillside\", \"slum\", \"earth movement\",\n",
    "    \"soil erosion\", \"slope\"\n",
    "]\n",
    "\n",
    "portuguese_terms = [\n",
    "    \"desastre natural\", \"enchente\", \"deslizamento de terra\", \"brasil\", \"deslizamento de lama\",\n",
    "    \"deslizamento de rochas\", \"trag√©dia\", \"emerg√™ncia\", \"chuva forte\", \"tempestade\",\n",
    "    \"estrada bloqueada\", \"evacuar\", \"cat√°strofe\", \"perigo\", \"crise\", \"busca e resgate\",\n",
    "    \"zona de desastre\", \"fechamento de estrada\", \"rodovia fechada\", \"resposta de emerg√™ncia\",\n",
    "    \"ajuda humanit√°ria em casos de desastres\", \"intenso\", \"torrencial\", \"forte\", \"extremo\",\n",
    "    \"√°guas da enchente\", \"inunda√ß√£o repentina\", \"colapso\", \"destrui√ß√£o\", \"dano\", \"ferido\",\n",
    "    \"morto\", \"baixas\", \"ausente\", \"fatalidades\", \"montanha\", \"encosta\", \"favela\",\n",
    "    \"movimento da terra\", \"eros√£o do solo\", \"declive\"\n",
    "]\n",
    "\n",
    "spanish_terms = [\n",
    "    \"desastre natural\", \"inundaci√≥n\", \"corrimiento de tierras\", \"brasil\", \"avalancha de lodo\",\n",
    "    \"deslizamiento de rocas\", \"tragedia\", \"emergencia\", \"lluvia pesada\", \"tormenta\",\n",
    "    \"carretera bloqueada\", \"evacuar\", \"cat√°strofe\", \"peligro\", \"crisis\", \"b√∫squeda y rescate\",\n",
    "    \"zona de desastre\", \"cierre de carretera\", \"autopista cerrada\", \"respuesta de emergencia\",\n",
    "    \"socorro en casos de desastre\", \"intenso\", \"torrencial\", \"severo\", \"extremo\",\n",
    "    \"aguas de la inundaci√≥n\", \"inundaci√≥n repentina\", \"colapso\", \"destrucci√≥n\", \"da√±o\",\n",
    "    \"herido\", \"muerto\", \"bajas\", \"desaparecido\", \"muertes\", \"monta√±a\", \"ladera\", \"barrio bajo\",\n",
    "    \"movimiento de la tierra\", \"erosi√≥n del suelo\", \"pendiente\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Brazil) AND (\"natural disaster\" OR \"flood\" OR \"landslide\" OR \"brazil\" OR \"mudslide\" OR \"rockslide\" OR \"tragedy\" OR \"emergency\" OR \"heavy rain\" OR \"storm\")\n",
      "---\n",
      "(Brazil) AND (\"road blocked\" OR \"evacuate\" OR \"catastrophe\" OR \"hazard\" OR \"crisis\" OR \"search and rescue\" OR \"disaster zone\" OR \"road closure\" OR \"highway closed\" OR \"emergency response\")\n",
      "---\n",
      "(Brazil) AND (\"disaster relief\" OR \"intense\" OR \"torrential\" OR \"severe\" OR \"extreme\" OR \"floodwaters\" OR \"flash flood\" OR \"collapse\" OR \"destruction\" OR \"damage\")\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def chunk_query_terms(terms, chunk_size=10):\n",
    "    \"\"\"Yield chunks of terms to keep queries short and API-safe.\"\"\"\n",
    "    for i in range(0, len(terms), chunk_size):\n",
    "        yield terms[i:i + chunk_size]\n",
    "\n",
    "def build_query(base, terms):\n",
    "    \"\"\"Builds a Reddit search query like (brazil) AND (term1 OR term2 ...)\"\"\"\n",
    "    combined = \" OR \".join(f'\"{t}\"' for t in terms)\n",
    "    return f\"({base}) AND ({combined})\"\n",
    "\n",
    "# Combine all three language term lists\n",
    "all_terms = english_terms + portuguese_terms + spanish_terms\n",
    "\n",
    "# Iterate through chunks to generate multiple safe queries\n",
    "queries = [build_query(\"Brazil\", chunk) for chunk in chunk_query_terms(all_terms, chunk_size=10)]\n",
    "\n",
    "# Print a few to preview\n",
    "for q in queries[:3]:\n",
    "    print(q)\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"brazil_landslides_reddit.csv\"\n",
    "SAVE_EVERY = 5   # how many batches before saving\n",
    "BATCH_SIZE = 100 # posts per subreddit per window\n",
    "SLEEP_BETWEEN_CALLS = 2  # seconds between API calls\n",
    "\n",
    "QUERY = \"(Brazil AND ((landslide OR mudslide)) OR (deslizamento AND terra))\"\n",
    "SUBREDDITS = [\"worldnews\", \"news\", \"brazil\", \"environment\", \"earthscience\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Loaded 44 existing posts from brazil_landslides_reddit.csv\n"
     ]
    }
   ],
   "source": [
    "seen = set()\n",
    "if os.path.exists(DATA_FILE) and os.path.getsize(DATA_FILE) > 0:\n",
    "    try:\n",
    "        existing = pd.read_csv(DATA_FILE)\n",
    "        seen.update(existing[\"id\"].astype(str))\n",
    "        print(f\"üîÅ Loaded {len(seen)} existing posts from {DATA_FILE}\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"‚ö†Ô∏è Empty file found, starting fresh.\")\n",
    "else:\n",
    "    print(\"üìÅ No existing data found ‚Äî starting fresh.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_reddit_batch(start_date, end_date, query, subreddits):\n",
    "    posts = []\n",
    "\n",
    "    for sub in subreddits:\n",
    "        print(f\"üîé Searching r/{sub}...\")\n",
    "        try:\n",
    "            for post in reddit.subreddit(sub).search(query, sort=\"new\", limit=BATCH_SIZE):\n",
    "                post_date = datetime.utcfromtimestamp(post.created_utc)\n",
    "                if start_date <= post_date <= end_date:\n",
    "                    if post.id not in seen:\n",
    "                        seen.add(post.id)\n",
    "                        posts.append({\n",
    "                            \"id\": post.id,\n",
    "                            \"title\": post.title,\n",
    "                            \"subreddit\": sub,\n",
    "                            \"url\": post.url,\n",
    "                            \"score\": post.score,\n",
    "                            \"num_comments\": post.num_comments,\n",
    "                            \"created_utc\": post.created_utc,\n",
    "                            \"created_date\": post_date.strftime(\"%Y-%m-%d\")\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error searching r/{sub}: {e}\")\n",
    "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
    "\n",
    "    print(f\"‚úÖ Collected {len(posts)} posts in this batch\")\n",
    "    return pd.DataFrame(posts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_collect_reddit(start_year=2015, end_year=2025):\n",
    "    all_batches = []\n",
    "    batch_count = 0\n",
    "\n",
    "    # Build all multilingual query chunks\n",
    "    all_terms = english_terms + portuguese_terms + spanish_terms\n",
    "    queries = [build_query(\"Brazil\", chunk) for chunk in chunk_query_terms(all_terms, chunk_size=10)]\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13, 3):  # 3-month windows\n",
    "            start_date = datetime(year, month, 1)\n",
    "            end_month = min(month + 2, 12)\n",
    "            last_day = 30 if end_month != 2 else 28\n",
    "            end_date = datetime(year, end_month, last_day, 23, 59, 59)\n",
    "\n",
    "            print(f\"\\nüìÜ Collecting {start_date.strftime('%b %Y')} ‚Äì {end_date.strftime('%b %Y')}\")\n",
    "\n",
    "            # loop through every multilingual query chunk\n",
    "            for query in queries:\n",
    "                print(f\"üîç Running query: {query[:80]}...\")\n",
    "                df_batch = collect_reddit_batch(start_date, end_date, query, SUBREDDITS)\n",
    "\n",
    "                if not df_batch.empty:\n",
    "                    all_batches.append(df_batch)\n",
    "                    batch_count += 1\n",
    "\n",
    "                # periodic save\n",
    "                if batch_count % SAVE_EVERY == 0 and all_batches:\n",
    "                    df_all = pd.concat(all_batches, ignore_index=True)\n",
    "                    if os.path.exists(DATA_FILE) and os.path.getsize(DATA_FILE) > 0:\n",
    "                        old = pd.read_csv(DATA_FILE)\n",
    "                        df_all = pd.concat([old, df_all], ignore_index=True).drop_duplicates(subset=[\"id\"])\n",
    "                    df_all.to_csv(DATA_FILE, index=False)\n",
    "                    print(f\"üíæ Saved {len(df_all)} total posts so far\")\n",
    "                    all_batches = []\n",
    "\n",
    "            # safety pause between 3-month windows\n",
    "            time.sleep(3)\n",
    "\n",
    "    # final save\n",
    "    if all_batches:\n",
    "        df_all = pd.concat(all_batches, ignore_index=True)\n",
    "        if os.path.exists(DATA_FILE) and os.path.getsize(DATA_FILE) > 0:\n",
    "            old = pd.read_csv(DATA_FILE)\n",
    "            df_all = pd.concat([old, df_all], ignore_index=True).drop_duplicates(subset=[\"id\"])\n",
    "        df_all.to_csv(DATA_FILE, index=False)\n",
    "        print(f\"\\n‚úÖ Finished: {len(df_all)} unique posts saved to {DATA_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÜ Collecting Jan 2015 ‚Äì Mar 2015\n",
      "üîç Running query: (Brazil) AND (\"natural disaster\" OR \"flood\" OR \"landslide\" OR \"brazil\" OR \"mudsl...\n",
      "üîé Searching r/worldnews...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch_collect_reddit(start_year\u001b[39m=\u001b[39;49m\u001b[39m2015\u001b[39;49m, end_year\u001b[39m=\u001b[39;49m\u001b[39m2025\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[25], line 21\u001b[0m, in \u001b[0;36mbatch_collect_reddit\u001b[0;34m(start_year, end_year)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m query \u001b[39min\u001b[39;00m queries:\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39müîç Running query: \u001b[39m\u001b[39m{\u001b[39;00mquery[:\u001b[39m80\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     df_batch \u001b[39m=\u001b[39m collect_reddit_batch(start_date, end_date, query, SUBREDDITS)\n\u001b[1;32m     23\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m df_batch\u001b[39m.\u001b[39mempty:\n\u001b[1;32m     24\u001b[0m         all_batches\u001b[39m.\u001b[39mappend(df_batch)\n",
      "Cell \u001b[0;32mIn[24], line 24\u001b[0m, in \u001b[0;36mcollect_reddit_batch\u001b[0;34m(start_date, end_date, query, subreddits)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     23\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m‚ö†Ô∏è Error searching r/\u001b[39m\u001b[39m{\u001b[39;00msub\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     time\u001b[39m.\u001b[39msleep(SLEEP_BETWEEN_CALLS)\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m‚úÖ Collected \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(posts)\u001b[39m}\u001b[39;00m\u001b[39m posts in this batch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(posts)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_collect_reddit(start_year=2015, end_year=2025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 ('brazilian-favela-urop-AQHwoFdT-py3.11')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24eaac1d74d6eef61291c51b2896758e70de1d7dcc43d2fab8692019e3715269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
