{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit API Notebook\n",
    "\n",
    "This notebook will be attempting to use reddit api to find examples of Brazilian landslides/natural disasters in place of twitter api not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import json, os\n",
    "from openai import OpenAI\n",
    "from secretcodes import reddit_secret, reddit_client, open_api_key\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = reddit_client,\n",
    "    client_secret=reddit_secret,\n",
    "    user_agent=\"brazilian_urop by u/haleyhernan\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"(Brazil AND (landslide OR mudslide)) OR (deslizamento AND terra)\"\n",
    "subreddits = [\"worldnews\", \"news\", \"brazil\", \"environment\", \"earthscience\"]\n",
    "\n",
    "posts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will attempt to run our API on a small working example to test if it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Searching r/worldnews...\n",
      "\n",
      "üîé Searching r/news...\n",
      "\n",
      "üîé Searching r/brazil...\n",
      "\n",
      "üîé Searching r/environment...\n",
      "\n",
      "üîé Searching r/earthscience...\n"
     ]
    }
   ],
   "source": [
    "for sub in subreddits: \n",
    "    print(f\"\\nüîé Searching r/{sub}...\")\n",
    "    for post in reddit.subreddit(sub).search(query, limit=20, sort=\"new\"):\n",
    "        posts.append({\n",
    "            \"subreddit\": sub,\n",
    "            \"title\": post.title,\n",
    "            \"score\": post.score,\n",
    "            \"url\": post.url,\n",
    "            \"num_comments\": post.num_comments,\n",
    "            \"created_utc\": post.created_utc\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subreddit                                              title  score  \\\n",
      "0  worldnews  Brazil: Landslides and flooding kill 60 in Rio...    464   \n",
      "1  worldnews  Brazil floods: death toll rises to 48 as lands...     41   \n",
      "2  worldnews  Heavy rain triggers flooding, landslides in Br...     33   \n",
      "3  worldnews  Death Toll From Floods, Landslides Rises to 36...     22   \n",
      "4  worldnews  Landslides after heavy rains in southern Brazi...     26   \n",
      "\n",
      "                                                 url  num_comments  \\\n",
      "0     https://www.bbc.com/news/articles/c0w03627kq4o            45   \n",
      "1  https://www.theguardian.com/world/2023/feb/23/...             2   \n",
      "2  https://www.abc.net.au/news/2023-02-20/brazil-...            10   \n",
      "3  https://www.telesurenglish.net/news/Death-Toll...             0   \n",
      "4  https://www.france24.com/en/americas/20221201-...             2   \n",
      "\n",
      "    created_utc  \n",
      "0  1.714859e+09  \n",
      "1  1.677124e+09  \n",
      "2  1.676920e+09  \n",
      "3  1.676902e+09  \n",
      "4  1.669935e+09  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(posts)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Saved results to brazil_landslides.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"brazil_landslides.csv\", index=False)\n",
    "print(\"\\n‚úÖ Saved results to brazil_landslides.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now know it works, let's test this on a specific example to find a specific landslide in Brazil using the dates of posts. \n",
    "\n",
    "By doing this, we can test to see if it feasible to use our data collection to find undocumented cases of landslides. \n",
    "\n",
    "Reddit API doesn't support date filtering, so we will be using Pushshift to try and filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Brazil (landslide OR mudslide OR flooding OR rain OR storm OR deslizamento OR enchente)\"\n",
    "start = datetime(2024, 10, 1).timestamp()\n",
    "end = datetime(2024, 10, 31, 23, 59, 59).timestamp()\n",
    "url = \"https://api.pullpush.io/reddit/search/submission/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"q\": query,\n",
    "    \"after\": after,\n",
    "    \"before\": before,\n",
    "    \"size\": 100,          \n",
    "    \"sort\": \"desc\",\n",
    "    \"sort_type\": \"score\"  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, params=params)\n",
    "data = response.json().get(\"data\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Searching r/worldnews...\n",
      "\n",
      "üîé Searching r/news...\n",
      "\n",
      "üîé Searching r/brazil...\n",
      "\n",
      "üîé Searching r/environment...\n",
      "\n",
      "üîé Searching r/earthscience...\n"
     ]
    }
   ],
   "source": [
    "for sub in subreddits:\n",
    "    print(f\"\\nüîé Searching r/{sub}...\")\n",
    "    for post in reddit.subreddit(sub).search(query, limit=200, sort=\"new\"):\n",
    "        if start <= post.created_utc <= end:\n",
    "            posts.append({\n",
    "                \"subreddit\": sub,\n",
    "                \"title\": post.title,\n",
    "                \"score\": post.score,\n",
    "                \"url\": post.url,\n",
    "                \"num_comments\": post.num_comments,\n",
    "                \"created_utc\": datetime.utcfromtimestamp(post.created_utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   subreddit                                              title  score  \\\n",
      "0  worldnews  Eight dead as heavy rain thrashes Brazil after...     61   \n",
      "1     brazil               Any suggestions north of Salvador ?       1   \n",
      "2     brazil                                Amazon travel tips?      2   \n",
      "3     brazil             Did I make a mistake booking my visit?      2   \n",
      "4     brazil                                         Sent to me    323   \n",
      "\n",
      "                                                 url  num_comments  \\\n",
      "0  https://phys.org/news/2024-10-dead-heavy-thras...             2   \n",
      "1  https://www.reddit.com/r/Brazil/comments/1gg9f...             4   \n",
      "2  https://www.reddit.com/r/Brazil/comments/1g016...             4   \n",
      "3  https://www.reddit.com/r/Brazil/comments/1fzmz...            15   \n",
      "4               https://i.redd.it/xe074bozqmsd1.jpeg            54   \n",
      "\n",
      "           created_utc  \n",
      "0  2024-10-14 04:41:10  \n",
      "1  2024-10-31 08:08:20  \n",
      "2  2024-10-09 20:24:26  \n",
      "3  2024-10-09 08:57:48  \n",
      "4  2024-10-03 23:58:55  \n",
      "\n",
      "‚úÖ Saved results to brazil_landslides_oct2024.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(posts)\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\"brazil_landslides_oct2024.csv\", index=False)\n",
    "print(\"\\n‚úÖ Saved results to brazil_landslides_oct2024.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will clean the data before feeding into the LLM. This will reformat the contents of the post to be uniformed. It will also remove duplicate posts/posts with very similar scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"brazil_landslides.csv\")\n",
    "\n",
    "df[\"title_clean\"] = df[\"title\"].str.lower().str.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(df[\"title_clean\"], convert_to_tensor=True)\n",
    "cosine_scores = util.cos_sim(embeddings, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = set()\n",
    "for i in range(len(df)):\n",
    "    if i in to_drop:\n",
    "        continue\n",
    "    for j in range(i + 1, len(df)):\n",
    "        if cosine_scores[i][j] > 0.8:\n",
    "            to_drop.add(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 22 similar posts, 24 remain.\n"
     ]
    }
   ],
   "source": [
    "df_clean = df.drop(df.index[list(to_drop)])\n",
    "df_clean.to_csv(\"brazil_landslides_clean.csv\", index=False)\n",
    "print(f\"Removed {len(to_drop)} similar posts, {len(df_clean)} remain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do LLM work to quickly turn each record into a JSON with it's location, date, type of disaster, severity, sentiment, and possible source type (news vs. personal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=open_api_key)\n",
    "\n",
    "def extract_event_info(title: str):\n",
    "    prompt = f\"\"\"\n",
    "    You are extracting event information from Reddit posts about Brazilian landslides.\n",
    "    For the post below, return a JSON with:\n",
    "    - \"location\": most likely location or city/state if mentioned\n",
    "    - \"event_type\": short phrase like \"landslide\", \"flood\", \"mudslide\"\n",
    "    - \"severity\": estimate low/medium/high based on death toll or language intensity\n",
    "    - \"sentiment\": classify as negative/neutral/positive (based on tone)\n",
    "    - \"source_type\": \"news\" if linking to a news outlet, otherwise \"personal/post\"\n",
    "    - \"summary\": a short 1-line summary\n",
    "\n",
    "    Post title: \"{title}\"\n",
    "\n",
    "    Return ONLY valid JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # Extract and safely parse JSON\n",
    "    try:\n",
    "        content = response.choices[0].message.content\n",
    "        data = json.loads(content)\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error parsing JSON for:\", title)\n",
    "        print(\"Response content:\", content)\n",
    "        data = {}\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error parsing JSON for: deadly landslide engulfs motorway in brazil\n",
      "Response content: ```json\n",
      "{\n",
      "    \"location\": \"Brazil\",\n",
      "    \"event_type\": \"landslide\",\n",
      "    \"severity\": \"high\",\n",
      "    \"sentiment\": \"negative\",\n",
      "    \"source_type\": \"personal/post\",\n",
      "    \"summary\": \"A deadly landslide has engulfed a motorway in Brazil.\"\n",
      "}\n",
      "```\n",
      "‚ö†Ô∏è Error parsing JSON for: dramatic footage shows moment brazil mudslide begins\n",
      "Response content: ```json\n",
      "{\n",
      "    \"location\": \"Brazil\",\n",
      "    \"event_type\": \"mudslide\",\n",
      "    \"severity\": \"medium\",\n",
      "    \"sentiment\": \"negative\",\n",
      "    \"source_type\": \"personal/post\",\n",
      "    \"summary\": \"Video captures the initial moments of a mudslide in Brazil.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for t in df_clean[\"title_clean\"]:\n",
    "    info = extract_event_info(t)\n",
    "    info[\"title\"] = t\n",
    "    results.append(info)\n",
    "\n",
    "events_df = pd.DataFrame(results)\n",
    "events_df.to_csv(\"brazil_landslides_structured.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = events_df.groupby(\"location\")\n",
    "\n",
    "summaries = []\n",
    "for location, group in grouped:\n",
    "    summary_prompt = f\"\"\"\n",
    "    Summarize the following reports about landslides/floods in {location}.\n",
    "    Include:\n",
    "    - The likely event date range\n",
    "    - Overall severity (low/medium/high)\n",
    "    - The nature of the events (landslide, flood, mudslide, etc.)\n",
    "    - The general sentiment (negative, neutral, or positive)\n",
    "    - Notable details (like number of posts, sources, or key facts)\n",
    "    \n",
    "    Posts data:\n",
    "    {group.to_json(orient='records')}\n",
    "    \"\"\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    summaries.append({\n",
    "        \"location\": location,\n",
    "        \"summary\": resp.choices[0].message.content\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(summaries)\n",
    "summary_df.to_csv(\"brazil_landslides_event_summaries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 ('brazilian-favela-urop-AQHwoFdT-py3.11')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24eaac1d74d6eef61291c51b2896758e70de1d7dcc43d2fab8692019e3715269"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
